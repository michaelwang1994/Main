{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Pattern Discovery Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Patterns and Association Rules\n",
    "* itemset: A set of one or more items\n",
    "* k-itemset: X = {x1, ..., xk}\n",
    "* (absolute) support (count) of X: Frequency or the number of occurrences of an itemset X\n",
    "* (relative) support, s: The fraction of transactions that contains X (i.e. the probability)\n",
    "* An itemset X is **frequent** if the support of X is no less than a **minsup** threshold (denoted as $\\sigma$)\n",
    "* Association rules: X --> Y (s, c)\n",
    "    * Support, s: the p that a transation contains X $\\cup$ Y\n",
    "    * Confidence, c: the conditional p that a transation containing X also contains Y\n",
    "        * c = sup(X $\\cup$ Y) / sup(X)\n",
    "* Association Rule Mining: Find all the rules, X --> Y with min sup and con\n",
    "\n",
    "## Challenge: There are too many frequent patterns!\n",
    "* Given TDB: T_1:{a_1,..., a_50}, T_2:{a_1,..., a_100}\n",
    "    * Solution 1: Closed patterns: A pattern (itemset) X is **closed** if X is **frequent** and there exists no **super-pattern** Y contains X, with the same **support** as X.\n",
    "        * Super patterns by definition will have lower or equal support than their patterns.\n",
    "            * ex: (a, b) has p = .5, (a, b, c) has p = .5. Not closed.\n",
    "            * ex: (a, b, c) has p = .5, (a, b, c, d) has p = .25. Closed.\n",
    "    * Solution 2: Max-patterns: A pattern X is **max-pattern** if X is frequent and there exits no frequent super-pattern Y contains X.\n",
    "        * This is very lossy because you may lose frequent patterns\n",
    "            * ex: (a, b) has p = .5, (a, b, c) has p = .4, both are frequent. Only (a, b, c) is the max-pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 2: Efficient Pattern Mining Methods\n",
    "\n",
    "## The Downward Closure Property of Frequent Patterns\n",
    "\n",
    "The **downward closure (aka \"Apriori\")** property of frequent patterns status any subset of a frequent itemset must be frequent. Likewise if any itemset is infrequent the **Apriori pruning principle** states that any itemset that's infrequent's superset shouldn't be included.\n",
    "\n",
    "** Apriori Algorithm **\n",
    "1. Scen DB once to get frequent 1-itemset\n",
    "2. Generate length-(k+1) candidate itemsets from length-k frequent it itemsets\n",
    "3. Test the candidates against DB to find frequent (k+1)-itemsets\n",
    "4. Set k := k + 1\n",
    "5. Repreat 2-4 until no frequent or candidate set can be generated\n",
    "6. Return all frequent itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apriori(data, f):\n",
    "    # data needs to be a pd.DataFrame in the following shape:\n",
    "    # tid set\n",
    "    # 0 {a, b, c}\n",
    "    \n",
    "    total_len = len(data)\n",
    "    pd.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the Apriori Algorithm:\n",
    "* **Partitioning**\n",
    "    * Theorem: Any itemset that is potentially frequent in TDB must be frequent in at least one of the partitions of TDB\n",
    "    * Method\n",
    "        * Scan 1: Partition db and find local frequent patterns\n",
    "            * You must put k partitions of the db into the main memory\n",
    "        * Scan 2: Consolidate global frequent patterns\n",
    "* **Direct Hashing and Pruning (DHP)**\n",
    "    * Candidates: a, b, c, d, e\n",
    "    * Hash entries\n",
    "        * {ab, ad, ae}\n",
    "        * {bd, be, de}\n",
    "        * ...\n",
    "    * Frequent 1-itemset: a, b, d, e\n",
    "    * ab is not a candidate 2-itemset if the sum of count of {ab, ad, ae} is below support threshold\n",
    "* **ECLAT (Exploring Vertical Data Format: ECLAT)**\n",
    "    * A DFS algo using set intersection\n",
    "    * Properties of Tid-Lists\n",
    "        * t(X) = t(Y): X and Y always happend together (e.g. t(ac) = t(d))\n",
    "        * t(X) in t(Y): transaction having X always has Y (e.g. (t(ac) in t(ce))\n",
    "        * diffset(ce, e) = t(20)\n",
    "            * only keeping diffsets can accelerage mining\n",
    "    * Vertical format:\n",
    "\n",
    "|item|TidList    |\n",
    "|----|:---------:|\n",
    "|  a |10, 20     |\n",
    "|  b |20, 30     |\n",
    "|  c |10, 30     |\n",
    "|  d |10         |\n",
    "|  e |10, 20, 30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Frequent Pattern Growth (FPGrowth)**\n",
    "\n",
    "    * Find frequent single items and partition the db based on each such item\n",
    "    * Recursively grow frequent patterns by doing the above for each partitioned db (aka conditional db)\n",
    "    * To facilitate efficient processing, an efficient data structure, **FP-tree**, can be constructed\n",
    "    * Mining becomes\n",
    "        * Recursively contruct and mine (conditional) FP-trees\n",
    "        * Until the resulting FP-tree is empty, or until it contains only one path -- single path will generate all the combinations of its sub-paths, each of which is a frequent pattern\n",
    "    * p's condition pattern base: ** transformed prefix paths ** of item p\n",
    "    * use parallel and partition project to scale FPGrowth\n",
    "    \n",
    "* **Mining Closed Patterns (CLOSET+)**\n",
    "    * so many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Pattern Evaluation\n",
    "\n",
    "## What is an interesting pattern?\n",
    "\n",
    "* Objective:\n",
    "    * Support, confidence, correlation, ...\n",
    "* Subjective:\n",
    "    * Query-based: relevant to a user's particular request\n",
    "    * Against one's knowledge-base\n",
    "    * Visualization tools\n",
    "\n",
    "## List and $\\chi^2$\n",
    "\n",
    "* Measure of dependent/correlated events: **lift**\n",
    "    * $ lift (B, C) = \n",
    "    \\dfrac{c(B \\implies C)}{s(C)}  = \n",
    "    \\dfrac{c(B \\cup C)}{s(B) \\times s(C)}$\n",
    "        * If list(B, C) = 1, then B and C are independent\n",
    "        * lift > 1: positively correlated\n",
    "        * lift < 1: negatively correlated\n",
    "* Interestingness Measure: $\\chi^2$\n",
    "    * $ \\chi^2 = \\sum \\dfrac{(Observed - Expected)^2}{Expected}$\n",
    "        * We calculate expected by looking at row sums.\n",
    "        * $\\chi^2$ = 0: independent\n",
    "        * $\\chi^2$ > 0: correlated, either positive or negative so it needs additional test... Just checked observed vs. expected.\n",
    "* **Null transactions**: transactions that contain neither B nor C\n",
    "\n",
    "## Null Invariance Measures\n",
    "\n",
    "* $\\chi^2$ and lift are not null-invariant\n",
    "* AllConf, Jaccard, Cosine, Kulczynski, MaxConf are null-invariant\n",
    "* Null invariance is important because many transaction sets that contain neither milk nor coffee, which will mess things up when you're trying to compaire milk vs. coffee.\n",
    "\n",
    "## Comparison of Null-Invariant Measures\n",
    "\n",
    "* While all five null-invariant measures are comparable when the dataset is very unbalanced, or very balanced.\n",
    "    * Kulc holds firm and is in balance of both direction imbalances\n",
    "    * $Kulczynski(A, B) = \n",
    "    \\frac{1}{2} \\big( \n",
    "    \\dfrac{s(A \\cup B)}{s(A)} + \\dfrac{s(A \\cup B)}{s(B)} \\big)$\n",
    "* Imbalance Ratio with Kulczynski Measure\n",
    "    * $ IR(A, B) = \\dfrac{|s(A) - s(B)|}{s(A) + s(B) - s(A \\cup B)}$\n",
    "* ** The best measure of correlation between dataset is a combination of Kulczynki and imbalance ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Mining Diverse Frequent Patterns\n",
    "## Mining Multi-Level Associations\n",
    "\n",
    "* Items often form heirarchies:\n",
    "    * Milk [support = 10%]\n",
    "        * 2% Milk [support = 6%]\n",
    "        * Skim Milk [support = 2%]\n",
    "* How to set min-support thresholds?\n",
    "    * Uniform min-support across multiple levels? \n",
    "    * **Level-reduced min-support**: Items are at the lower level are exepected to have lower support\n",
    "* Efficient mining: **Shared** multi-level mining\n",
    "    * Use the lower min-support to pass down the set of candidates\n",
    "* Multi-level association mining may have many reduncies\n",
    "    * If lower-level rule can be derived from high-level rule, we remove the low-level rule.\n",
    "* We can make groups-based 'individualized' min-support.\n",
    "\n",
    "## Mining Multi-Dimensional Associations\n",
    "\n",
    "* Single-dimensional rules:\n",
    "    * buys(X, 'milk') --> buys(X, 'bread')\n",
    "* Multi-dimensional rules:\n",
    "    * age(X, '18-25') ^ occupation(X, 'student') --> buys(X, 'coke')\n",
    "* Hybrid-dimensional rules:\n",
    "    * age(X, '18-25') ^ buys(X, 'popcorn') --> buys(X, 'coke')\n",
    "* For categorical data:\n",
    "    * We can use a data cube\n",
    "* For quantitative data:\n",
    "    * We use clustering, etc.\n",
    "    \n",
    "## Mining Quantitative Association\n",
    "\n",
    "* How do we mine numerical attributes?\n",
    "    * We need to perform **static discretization** based on predefined concept hierarchies\n",
    "    * We perform clustering to make distance-based association\n",
    "    * We can also do deviation analysis:\n",
    "        * e.g. For females, if mean(wage) is different from the all gender mean(wage), that can be interesting.\n",
    "        * Use statistical tests (Z-test)\n",
    "\n",
    "## Rare Patterns vs. Negative Patterns\n",
    "\n",
    "* Rare Patterns\n",
    "    * Very low support but interesting (e.g. buying Rolex)\n",
    "    * How to mine? Setting individualized, group-based min-support thresholds for different groups of items\n",
    "* Negative patterns\n",
    "    * Negatively correlated: unlikely to happen together (e.g. buying sports car and hybrid car)\n",
    "    * A support-based definition\n",
    "        * If itemsets A and B are both frequent but rarely occur together i.e. $sup(A \\cup B) << sup (A) \\times sup(B) $\n",
    "            * Then A and B are negatively correlated (lift?)\n",
    "        * This breaks down again because this is not null-invariant\n",
    "    * A Kulczynski measure-based definition\n",
    "        * If itemsets A and B are frequent but (P(A|B) + P(B|A)) / 2 < e where e is a negative pattern threshold\n",
    "        \n",
    "## Mining Compressed Patterns\n",
    "\n",
    "* There are often too many scattered patterns but not so many are meaningful\n",
    "    * We can just look at closed patterns, but there's too much of a use of support\n",
    "    * Max patterns meas that we lose too much information\n",
    "* We can use pattern distance measure\n",
    "    * $ Dist(P_1, P_2) = 1 - \\dfrac{T(P_1) \\cap T(P_2)}{T(P_1) \\cup T(P_2)} $\n",
    "    * $ \\delta$-clustering: For each pattern P, find all patterns which can be expressed by P and whose distance to P is within $ \\delta$ ($ \\delta$-cover)\n",
    "\n",
    "* Redundancy-aware top-k patterns\n",
    "    * Method: use MMS (Maximal Marginal Significance) for measuring the combined significance of a pattern set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 5: Sequential Pattern Mining\n",
    "\n",
    "## Sequential Patterns\n",
    "\n",
    "* This has broad applications:\n",
    "    * Customer shopping sequences\n",
    "    * Natural disasters, medical treatments, stocks etc.\n",
    "    * Click streams\n",
    "    * Program execution sequences,...\n",
    "    * Biological sequences: DNA, protein\n",
    "* Transaction DB, sequence DB vs. time-series DB\n",
    "* Gapped vs. non-gapped sequential patterns\n",
    "    * shopping, clicking streams can be gapped\n",
    "    * biological sequences do care about gaps\n",
    "* Sequential pattern mining: given a set of sequences, find the **complete set of frequent subsequences** (i.e. satisfying the min_sup threshold)\n",
    "    * e.g. A **sequence** is < (ef) (ab) (df) c b >\n",
    "        * An **element** may contain a set of **items** (a.k.a. **events**)\n",
    "        * Items within an element are unordered and we list them alphabetically\n",
    "* Algorithm requirement: efficient, scalable, finding complete set, incorporating varios kinds of user-specific constaints\n",
    "* The Apriori property still holds: if a subsequence s_1 is infrequent, none of s_1's super-sequences can be frequent\n",
    "* Representative Algorithms:\n",
    "    * GSP, SPADE, PrefixSpan, CloSpan, Constrain-based sequential pattern mining\n",
    "    \n",
    "## GSP: Apriori-Based Sequential Pattern Mining\n",
    "\n",
    "* Initial candidates: All singleton sequences\n",
    "* Scan DB once, count support for each candidate\n",
    "* Generate length-2 candidate sequences\n",
    "    * Need to add combos (e.g. (ab), (ac), etc.)\n",
    "    \n",
    "## SPADE - Sequential Pattern Mining in Vertical Data Format\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "* Turn transaction DB into vertical series (like my data currently)\n",
    "* Check which ticks contain \"a\", \"b\", etc.\n",
    "* Check which ticks contain \"a\" and then which ticks contain \"b\"\n",
    "* Combine for bigger patterns\n",
    "\n",
    "## PrefixSpan - A Pattern-Growth Approach\n",
    "\n",
    "* **Prefix**: a sub-sequence that looks at the first elements of a sequence. A placeholder is inserted in the **suffix**, or the remaining subsequence, if one element contains other items.\n",
    "    * e.g. pattern: < a(abc)(ac)d(cf) >, prefix: < aa >, suffix: < (_bc)(ac)d(cf) >\n",
    "\n",
    "* PrefixSpan Mining:\n",
    "    * Step 1: Find lenght-1 sequential patterns\n",
    "    * Step 2: Divid search space and mine each projected DB\n",
    "* Strengths:\n",
    "    * No candidate subsequences to be generated\n",
    "    * Projected DBs keep shrinking\n",
    "* Costs: Constructing projected DBs\n",
    "    * Suffixes are lergely repreating in recursive projected DBs\n",
    "    * When DB can be held in main memory, use pseudo projection\n",
    "        * No physical copying suffixes\n",
    "        * Pointer to the sequence (e.g. s|< a >:(, 2) points to < (abc)(ac)d(cf) >\n",
    "        * Offset of the suffix\n",
    "    * If it does not fit in memory\n",
    "        * Physical projection\n",
    "    * Suggested approach: \n",
    "        * Swap to pseudo-projection when the data fits in memory.\n",
    "        \n",
    "## CloSpan: Mining Closed Sequential Patterns\n",
    "\n",
    "* A **closed sequential pattern** s: There exists no superpattern s' s.t. s' contains s, and s' and s have the same support\n",
    "    * e.g. < abc >: 20, < abcd >: 20 so the latter is closed.\n",
    "* Why directly mine closed sequential patterns? Same reason we mine close non-sequential patterns.\n",
    "    * We can use **backward subpattern** and **backward superpattern** pruning to prune redundant search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
