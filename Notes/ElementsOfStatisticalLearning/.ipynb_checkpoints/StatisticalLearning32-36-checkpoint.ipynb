{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection and Best Subset Selection (32)\n",
    "\n",
    "Recall the LM:\n",
    "\n",
    "$$ Y = \\beta_{0} + \\beta_{1}X_{1} + ... + \\beta_{p}X_{p} + \\epsilon $$\n",
    "\n",
    "We will consider **non-linear** and **additive** models. \n",
    "\n",
    "We will look at alternative to least squares for the sake of:\n",
    "\n",
    "* **Prediction Accuracy**: especially when p > n, to control the variance\n",
    "* **Model Interpretability**: by removing irrelevant features, it is more useful and deliverable to others.\n",
    "\n",
    "Three classes:\n",
    "\n",
    "* **Subset selection**: identify subsets of p predictors that are good.\n",
    "* **Shrinkage**: penalize the model for size of coefficients\n",
    "* **Dimension Reduction**: finding combinations of variables\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "1. Let $\\mathcal{M}_{0}$ denotes the **null model**, which contains no predictors. This model simply predicts the sample mean for each observation.\n",
    "2. For $k = 1, 2, ... p$:\n",
    "    a. Fit all $\\binom {p}{k}$ models that contain exactly k predictors.\n",
    "    b. Pick the best among these $\\binom{p}{k}$ models, and call it $\\mathcal{M}_{k}$. Here **best** is definded as having the smallest RSS, or largest $R^2$.\n",
    "3. Select a single best model from among $\\mathcal{M}_{0}, ..., \\mathcal{M}_{p}$ using cross-validated, prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$.\n",
    "    * we need to choose the one with the best **test error** not **training error**.\n",
    "\n",
    "# Forward Stepwise Selection (33)\n",
    "\n",
    "* For computational reasons, best subset selection cannot be applied with very large p because you'll have $2^p$ models. Beyond 40 predictors, best subset selection stops working.\n",
    "* Best subset selection may also suffer from statistical problems with overfitting because it'll choose the model that look the best on the training data.\n",
    "    * This is counterintuitive because we're used to doing things the most exact.\n",
    "    \n",
    "### Forward Stepwise Selection\n",
    "\n",
    "* Begins with a model containing no predictors and then adds predictors to the model, one-at-a-time, until all predictors are in the model.\n",
    "    * Each variable that gives the greatest **additional** improvement the the model.\n",
    "    \n",
    "1. Let $\\mathcal{M}_{0}$ denote the null model, which contains no predictors.\n",
    "2. For $k = 0, ... , p -1$:\n",
    "    a. Consider all $p - k$ models that augment the predictors in $\\mathcal{M}_{k}$ with one additonal predictor.\n",
    "    b. Choose the **best** among these $p - k$ models, and call it $\\mathcal{M}_{k + 1}$. Here **best** is defined as having smallest RSS or highest $R^2$\n",
    "3. Select a single best model from among $\\mathcal{M}_{0},...,\\mathcal{M}_{p}$ using cross-validated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$.\n",
    "    * based on # of predictors in model\n",
    "\n",
    "# Backward Stepwise Selection (34)\n",
    "\n",
    "Opposite of forward\n",
    "\n",
    "1. Let $\\mathcal{M}_{p}$ denote the **full** model, which contains all p predictors.\n",
    "2. For $ k = p, p - 1, ..., 1$:\n",
    "    a. Consider all $k$ models that contain all but one of the predictors in $\\mathcal{M}_{k}$, for a total of $ k - 1$ predictors.\n",
    "    b. Choose the **best** among these k models, and call it $\\mathcal{M}_{k-1}$. Here **best** is defined as having smallest RSS or highest $R^2$\n",
    "3. Select a single best model from among $\\mathcal{M}_{0}, ..., \\mathcal{M}_{p}$ using cross-validated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$.\n",
    "\n",
    "# Estimating Test Error Using Mallow's Cp, AIC, BIC, Adjusted R-squared (35)\n",
    "\n",
    "We can indirectly estimate test error by making an **adjustment** to the training error to account for the bias due to overfitting. The above techniques all adjust the training error for the model size, and can be used to select among a set of models with different number of variables.\n",
    "\n",
    "### Mallow's Cp\n",
    "\n",
    "Defined by:\n",
    "\n",
    "$$ C_{p} = \\dfrac{1}{n} (RSS + 2d\\hat{\\sigma}^2) $$\n",
    "\n",
    "where d is the total # of parameters used and $\\hat{\\sigma}^2$ is an estimate of the variance of error $\\epsilon$ associated with each response measurement. \n",
    "\n",
    "### AIC (Akaike Information Criterion)\n",
    "\n",
    "$$ \\text{AIC} = -2\\log L + 2 d $$\n",
    "\n",
    "Where L is the maximized value of the likelihood function for the estimated model.\n",
    "\n",
    "* AIC and Cp are proportional so just use Cp.\n",
    "\n",
    "### BIC (Bayesian Information Criterion)\n",
    "\n",
    "$$ \\text{BIC} = \\dfrac {1}{n} (\\text{RSS} + \\log (n) d \\hat{\\sigma}^2) $$\n",
    "\n",
    "* The only difference between BIC and AIC is the 2 in from the log n.\n",
    "* BIC will put more of a penalty than AIC on large models.\n",
    "\n",
    "### Adjusted R-squared\n",
    "\n",
    "$$ \\text{Adjusted } R^2 = 1 - \\dfrac {\\text{RSS} / (n - d - 1)}{\\text{RSS} / (n - 1)} $$\n",
    "\n",
    "* Compared to regular R-squared, this error term punishes larger models\n",
    "* This is the fan favorite but statisticians like the previous because there's more theory behind it.\n",
    "\n",
    "# Estimating Test Error Using Cross-Validation (36)\n",
    "\n",
    "Cross validation/ Validation on $\\mathcal{M}_{k}$ will have some advantage over AIC, BIC, Cp, and R-squared because we will not need $\\sigma^2$.\n",
    "\n",
    "### One SD Rule\n",
    "\n",
    "Don't choose the minimum of SE, but choose the simplest model within one SD of the minimum SE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
