{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees (52)\n",
    "\n",
    "Given stratified data on baseball players' salaries based on hits:\n",
    "* Tree is split by players < 4.5 years\n",
    "    * Tree is further split by Hits < 117.5\n",
    "\n",
    "We interpret this tree by what's significant (years then # hits). After you've split by years, hits is important for people with >= 4.5 years of exp, but not for those with less.\n",
    "\n",
    "### Details\n",
    "\n",
    "1. We divide the predictor space into J distince and non-overlapping regions, $R_1, R_2, ..., R_J$.\n",
    "2. For every observation that falls in region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.\n",
    "\n",
    "The goal is to find boxes that **minimize the RSS**, given by:\n",
    "\n",
    "$$ \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_{j}}) ^ 2 $$\n",
    "\n",
    "Since looking at many boxes is computationally expensive, it uses \"top-down, greedy\" approach that is known as recursive binary splitting.\n",
    "\n",
    "Now when do we stop?\n",
    "\n",
    "# Pruning a Decision Tree (53)\n",
    "\n",
    "The good idea is to prune a large tree from the bottom until RSS is a good amount. We use **Cost complexity pruning** aka **weakest link pruning**. We consider a sequence of treesed indexed by tuning parameter $\\alpha$. For each $\\alpha$ there exists a subtree $T \\in T_0$ s.t.\n",
    "\n",
    "$$ \\sum_{m=1}^{|T|} \\sum_{x_i \\in R_m} (y_i - \\hat{y}_{R_m}) ^ 2 + \\alpha |T| $$\n",
    "\n",
    "The tuning param alpha controls a trad=off between a bias-variance. We select an optimal value $\\hat{\\alpha}$ using CV then obtain the subtree corresponding to $\\hat{\\alpha}$.\n",
    "\n",
    "# Classification Trees and Comparison with Linear Models (54)\n",
    "\n",
    "In classification trees, all we need to do is look at the **classification error rate** which is:\n",
    "\n",
    "$$ E = 1 - \\max_k (\\hat{p}_{mk}) $$\n",
    "\n",
    "However, that's very chunky and leads to bad tree growing. Another way is:\n",
    "\n",
    "### Gini index and Deviance\n",
    "\n",
    "$$ G = \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk}) $$\n",
    "\n",
    "The **Gini index** is a measure of total var across K classes. This is known as the **purity** index.\n",
    "\n",
    "An alternative is:\n",
    "\n",
    "$$ D = -\\sum_{k=1}^{K} \\hat{p}_{mk} \\log \\hat{p}_{mk} $$\n",
    "\n",
    "Which is **deviance** or **cross-entropy**.\n",
    "\n",
    "While trees are easy, fast, effective, it's not as accurate as the previous methods we talked about before.\n",
    "\n",
    "# Bootstrap Aggregation (Bagging) and Random Forests (55)\n",
    "\n",
    "** Bootstrap aggregation / bagging** is a general-purpose procedure for reducing the variance of methods. Recall that the variance is $\\sigma ^ 2$, and averaging a set of observations reduces variance.\n",
    "\n",
    "Instead we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate B different bootstrapped training data sets. We then train our method on the bth bootstrapped training set in order to get $\\hat{f}^{*b}(x)$, the prediction at a point x. Then we average these to obtain:\n",
    "\n",
    "$$ \\hat{f}_{bag}(x) = \\frac {1}{B} \\sum_{b=1}^{B} \\hat{f}^{*b}(x) $$\n",
    "\n",
    "This is called **bagging**.\n",
    "\n",
    "* Don't prune bushy trees, average them to reduce variance.\n",
    "\n",
    "### Out-of-Bag Error Estimation (OOB)\n",
    "\n",
    "This is essentially LOOCV error for bagging. A lot more effective than regular bagging.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "**Random Forests** provide an improvement over bagged trees to **decorrelate** the trees.\n",
    "\n",
    "* You take a random selection of m predictors at every split. And then make many trees, so then it will reappear in another tree.\n",
    "\n",
    "# Boosting and Variable Importance (56)\n",
    "\n",
    "1. Set $\\hat{f}(x) = 0$ and $r_i = y_i$ for all $i$ in the training set.\n",
    "2. For $b = 1, 2, ..., B, $ repeat: \n",
    "\n",
    "    a. Fit a tree $\\hat{f}^b $ with $d$ splits ($d + 1$ terminal nodes) to the training data $(X, r)$.\n",
    "    \n",
    "    b. Update $\\hat{f}$ by adding in a shrunken version of the new tree: $$ \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x) $$\n",
    "    c. Update the residuals: $$r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i) $$\n",
    "3. Output the boosted model, $$ \\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^b(x) $$\n",
    "\n",
    "The idea is to fit very slowly and shrink the residuals very slowly. `gbm` will allow boosting for classification.\n",
    "\n",
    "### Tuning parameter for boosting\n",
    "\n",
    "1. **The number of trees**. Boosting can overfit is the #, B, it too large\n",
    "2. The **shrinkage parameter** $\\lambda$. If this were too small, you'd need a large number B for this to work.\n",
    "3. The **number of splits** d. Usually d = 1 works well, but if d > 1, it's refered to as **depth**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-Labs, Decision Trees (57), Random Forest/Boosting (58)\n",
    "\n",
    "http://rpubs.com/hmwang/166483"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
